{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWaMPMXBUqQGLKcJ+v2x68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/podschwadt/teaching/blob/master/intro/tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWXqACbV1Au_",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow 1\n",
        "\n",
        "This a demonstration of how to use the Tenserflow 1 graph. How to build it and how to execute the graph. For more indepth info on the graph in TF 1 check here: https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/graphs.md \n",
        "\n",
        "In short. TF 1 handles its program in two steps. \n",
        "\n",
        "\n",
        "1.   Defining the graph and all the operations. Varialbe input into the graph is defined by empty placeholders.\n",
        "2.   Executing the graph using a session. This allows us to get results and feed data into the place holders.\n",
        "\n",
        "Creating a GAN using Tensorflow.\n",
        "\n",
        "Using two networks, a generator **G** and discrimantor **D** we want to create fake instances from the MNIST data set. \n",
        "\n",
        "**G** takes some noise $z$ and creates a fake from it.\n",
        "\n",
        "**D** trys to distinguish between fakes created by **G** and real samples from \n",
        "the training data.\n",
        "\n",
        "The classic GAN loss is defined as:\n",
        "$$\n",
        "\\min_G \\max_D V(D, G)=\n",
        "\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)]\n",
        "+ \\mathbb{E}_{z\\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIvvlS5w2joY",
        "colab_type": "text"
      },
      "source": [
        "Define a number of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APDYugY_RuaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.1\n",
        "NOISE_LENGTH = 10\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Imsl3Jx2of3",
        "colab_type": "text"
      },
      "source": [
        "Set up plotting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlInreZylCOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image( img ):\n",
        "    plt.imshow( img.reshape( 28, 28 ), cmap=\"gray_r\" )\n",
        "    plt.axis( 'off' )\n",
        "    plt.show( )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya-y8OCG2r7B",
        "colab_type": "text"
      },
      "source": [
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwC2FEFtsEBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "x_train = x_train.reshape( x_train.shape[ 0 ], -1 )\n",
        "\n",
        "print( x_train.shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-iyRnaG2uxI",
        "colab_type": "text"
      },
      "source": [
        "Define the Discriminator and Generator graph. Due to the way how Tensorflow 1 handles variables we are building a convience function to select all varibels that have a specific naming scheme.\n",
        "\n",
        "Reusing variables means that we do not create new subgraphs every time the function is called but that we reuse what we created perviously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rkTvyDRgzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to retrieve the variables with a specific name\n",
        "def variables_with_name( name ):\n",
        "    return [ x for x in tf.trainable_variables( ) if name in x.name ]\n",
        "\n",
        "def Disctriminator( inputs ):\n",
        "    with tf.variable_scope( '', reuse=tf.AUTO_REUSE ) as scope:\n",
        "        inputs = tf.layers.dense( name='Disc.1', units=256, activation=tf.nn.relu, inputs=inputs )\n",
        "        inputs = tf.layers.dense( name='Disc.2', units=1, activation=tf.nn.sigmoid, inputs=inputs )\n",
        "\n",
        "        return inputs\n",
        "\n",
        "def Generator( noise ):\n",
        "    with tf.variable_scope( '', reuse=tf.AUTO_REUSE ) as scope:\n",
        "        print( noise.shape )\n",
        "        output = tf.layers.dense( name='Gen.1', units=256, activation=tf.nn.relu, inputs=noise )\n",
        "        output = tf.layers.dense( name='Gen.2', units=x_train.shape[ 1 ], activation=tf.nn.sigmoid,\n",
        "                                  inputs=output )\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beBoTo_WMkYk",
        "colab_type": "text"
      },
      "source": [
        "Defining input place holders for both the noise and real data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZXo_6uxMpD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real = tf.placeholder( tf.float32, shape=[ BATCH_SIZE, x_train.shape[ 1 ] ] )\n",
        "noise_input = tf.placeholder( tf.float32, shape=[ BATCH_SIZE, NOISE_LENGTH ] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk9W767wMrQk",
        "colab_type": "text"
      },
      "source": [
        "Next we set up the graph that calcualtes the loss for the Generator and Discriminator as well as the optimizers to minimize that loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F5ksLcsQOCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tf.variable_scope( '', reuse=tf.AUTO_REUSE ) as scope:\n",
        "\n",
        "    # run and update generator\n",
        "    fake = Generator( noise_input )\n",
        "    gen_los = tf.reduce_mean( tf.log( Disctriminator( fake ) ) )\n",
        "    gen_params = variables_with_name( 'Gen' )\n",
        "    gen_trainer = tf.train.AdamOptimizer( learning_rate=LEARNING_RATE ).minimize( gen_los, var_list=gen_params )\n",
        "    \n",
        "    # run and update discriminator\n",
        "    fake_pred = Disctriminator( fake )\n",
        "    real_pred = Disctriminator( real )\n",
        "\n",
        "    disc_loss = tf.reduce_mean( -1. * tf.log( 1. - real_pred ) - tf.log( fake_pred ) )\n",
        "    disc_params = variables_with_name( 'Disc' )\n",
        "    disc_trainer = tf.train.AdamOptimizer( learning_rate=LEARNING_RATE ).minimize( disc_loss, var_list=disc_params )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m99uuPyaNLVI",
        "colab_type": "text"
      },
      "source": [
        "Now we can use a session to exectute the parts of the graphs that perform training. To this we use the run `methode`. It takes a list or a single what is called fetches. This means graph elements that we want to get the output for. And it needs a `feed_dict`. The `feed_dict` is a dictornay that contains values for all the placeholders that are required to exectute that graph the `fetches` depend on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtAvSGI10XLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # training loop\n",
        "    session = tf.Session( )\n",
        "    session.run( tf.global_variables_initializer( ) )\n",
        "    for epoch in range( EPOCHS ):\n",
        "        np.random.shuffle( x_train )\n",
        "        num_batches = x_train.shape[ 0 ] // BATCH_SIZE\n",
        "        for i in range( num_batches ):\n",
        "            batch = x_train[ i * BATCH_SIZE : (i+1) * BATCH_SIZE ]\n",
        "            # print( \"batch: \", batch.shape )\n",
        "            # noise z\n",
        "            z = np.random.normal( size=noise_input.shape )\n",
        "            session.run( [ disc_loss, disc_trainer ], feed_dict={ real: batch, noise_input: z } )\n",
        "            session.run( [ gen_los, gen_trainer ], feed_dict={noise_input: z} )\n",
        "        print ( \"epoch: \", epoch )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txeOAFknyMeH",
        "colab_type": "text"
      },
      "source": [
        "## Create some fakes\n",
        "\n",
        "Feed a noise vector to the generator and print the results. It might not looks line anything. This is ok simple GANs are quite finicky to train. You can try and change some of the hyperparameters and see who the results change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbbx3jOKyYXu",
        "colab_type": "text"
      },
      "source": [
        "## Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBkEJdEeyLq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( noise_input.shape )\n",
        "\n",
        "# noise z\n",
        "z = np.random.normal( size=noise_input.shape )\n",
        "\n",
        "result = session.run( fake , feed_dict={ noise_input: z }  )\n",
        "\n",
        "for x in result:\n",
        "  show_image( x )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}